{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP(Natural Language Processing)는 자연어처리를 뜻하며, AI가 인간의 언어를 처리할 수 있도록 하는 것이 목적인 인공지능이다.\n",
    "\n",
    "NLP의 주요 과제들의 예시로는 아래와 같이 나타낼 수 있다.\n",
    "\n",
    "1. Text Classification\n",
    "- 주어진 text를 범주 또는 카테고리로 분류 (ex) spam mail filtering, news article classification\n",
    "2. Named Entity Recognition\n",
    "- 주어진 text 내에서 개체명을 식별하고, 유형을 추출하는 작업 (ex) text 내의 인물의 이름 추출\n",
    "3. Text Summarization\n",
    "- 긴 text를 요약하여 핵심만 표현\n",
    "4. Sentiment Analysis\n",
    "- text에 나타나는 감정이나 의견 판별"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is NLP hard?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ambiguity\n",
    "- 단어의 중의성이나 문장 내 정보의 부족으로 인한 모호성이 존재\n",
    "(ex) 차를 마시러 공원에 가던 차 안에서 나는 그녀에게 차였다.\n",
    "(ex) 선생님은 울면서 돌아오는 우리를 위로했다.\n",
    "2. Various phrases\n",
    "- 하나의 상황을 설명하는데 있어서 다양한 표현이 존재"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 이유들 때문에, 자연어를 컴퓨터가 해석하기 쉽게 만들어주는 과정은 필수적이다.  \n",
    "컴퓨터가 해석할 수 있는 data는 기본적으로 binary data이므로, 인간의 언어를 binary data로 변환시키는 과정이 필요하다.  \n",
    "또한, 언어의 양은 정말 방대하기 때문에, 효율적으로 memory를 관리하면서 단어들간의 상관관계를 적절히 표현하는 일종의 압축 과정이 필요하다.  \n",
    "--> 이러한 과정을 embedding이라고 한다.\n",
    "\n",
    "[embedding의 필요한 조건]\n",
    "1. 많은 양의 data를 표현할 수 있어야 한다.\n",
    "2. 단어들간의 상관관계를 표현할 수 있어야 한다.\n",
    "\n",
    "--> 단어를 vector로 생각하고, inner product space이므로 기하적인 관계를 통해 상관관계를 파악할 수 있다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "**\"A word is characterized by the company it keeps!\"**\n",
    "**\"비슷한 분포를 가진 단어는 비슷한 의미를 가진다\"**\n",
    "\n",
    "Word2Vec은 2013년 Google에서 개발된 word embedding algorithm이며, 현재까지도 가장 널리 사용되는 embedding 방식 중 하나이다. \n",
    "\n",
    "Large corpus가 주어지면, 단어들간의 distribution을 학습함으로써 word를 n-dimensional vector space로 embedding할 수 있다는 것이 근본적인 아이디어이며, 이는 같은 맥락에서 나타나는 단어들은 의미적으로 연관성을 공유한다는 것을 활용한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장들이 주어지면, 첫번째 단계는 문장을 tokenize 하는것이다.\n",
    "이후, Word2Vec model은 token들을 입력으로 받아서 아래와 같은 구조를 가진다.\n",
    "\n",
    "\n",
    "![structure of Word2Vec](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fne3ld%2FbtqB6ikSExX%2F0SY9PsFIjugN8iav0zyZFk%2Fimg.png)\n",
    "\n",
    "Word2Vec은 CBOW와 Skip-Gram의 크게 2가지의 모델을 사용한다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW(Continuous Bag Of Word)\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbTF4tu%2FbtqErKyNHfS%2FNUbfNSKkCF2ktHwVleDknK%2Fimg.png\" width = \"500\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위 그림은 CBOW model의 구조이다. Tokenize가 이루어진 후, token들이 input으로 들어가게 된다.  \n",
    "총 V개의 distinct한 token이 있다고 하면, 각각의 token은 one-hot encoding을 통해 1 by V의 vector로 encoding이 이루어진다.  \n",
    "\n",
    "* 이 신경망의 각 token에 대한 Hidden Layer(Projected Layer)가 embedding의 결과이며, 이를 학습시키는 과정은 V by N matrix W와 N by V matrix W'을 학습시키는 것이다.  \n",
    "\n",
    "* Token들의 sequence를 sliding window를 사용하여 움직이며, 중심단어 기준 양옆 C/2개의 단어를 확인하여, 총 C개의 단어를 확인한다.  \n",
    "(왼쪽을 볼때는 max(k-C/2,0)의 index부터, 오른쪽을 볼때는 min(k+C/2, length)의 index까지를 확인한다.) (즉, 양끝에서는 C개가 아닐수도 있음)  \n",
    "\n",
    "* N은 embedding을 하는 vector의 차원으로, hyperparameter이다.  \n",
    "  * N의 값이 너무 작으면 모든 정보를 다 담을 수 없음  \n",
    "  * N의 값이 너무 크면 model becomes to complex, overfitting의 가능성이 존재  \n",
    "  -> 적당한 N의 값을 tuning하는 것도 주요 과제이다.  \n",
    "\n",
    "* Hidden Layer은 C개의 Input vector v에 대하여 Wv의 average값으로 결정된다.  \n",
    "\n",
    "* Hidden Layer에 W'을 곱해서 output을 얻고, softmax 함수로 normalize해준다.  \n",
    "  Output의 desired result는  target token의 one-hot-encoded vector이며, 이에 따라 output과 token의 one-hot-encoded vector과의 cross-entropy를 loss function으로 사용한다.  \n",
    "\n",
    "* 이를 통해 W와 W'을 학습시키며, 결과적으로 각 target token의 embedded result는 각 target token에 대한 Hidden Layer이 된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdLFuDz%2Fbtrcb9yhUhU%2FDs2wTl7jV3opmnJ5KK6kH0%2Fimg.png\" width = \"500\"/></center>  \n",
    "위는 CBOW model의 loss funtion이다.  \n",
    "* 위에서 설명한 것과 같이 softmax의 결과와 cross-entropy error을 사용하는 것을 확인할 수 있다.  \n",
    "(v hat는 hidden layer의 vector, w_c는 정답 vector을 뜻하며, u_j는 W'의 j번째 열을 의미한다.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwK7IS%2FbtqCaRMi3KE%2FTTqnMqzRW0sGTn4niPwKYK%2Fimg.png\" width = \"500\"/></center> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Skip-Gram은 CBOW와 달리 하나의 token을 기준으로 삼으며, 이를 input으로 신경망에 넣어준 후, 이를 통해 문맥 단어를 예측하는 형태로 학습이 이루어진다.  \n",
    "* 이는 CBOW가 하나의 token의 기준으로 삼고, 문맥 단어를 input으로 하여 기준 단어를 예측하는 형태로 학습이 이루어지는 것이라는 점에서 반대된다.  \n",
    "\n",
    "* 기준 token에 대한 1 by V one-hot-encoded vector를 신경망의 input으로 넣어준 후, V by N matrix W와 multiplication을 통해 1 by Nl Hidden Layer를 만든다.\n",
    "* 이후, C개의 N by V matrix W'를 곱하여 C개의 1 by V vector를 만들어낸다.  \n",
    "  이후 softmax 함수를 통과시켜 normalize를 진행한 후 각 token의 true one-hot-encoded vector과 cross entropy들의 합을 loss function으로 생각하여 최적화를 진행시킨다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbsIXcB%2Fbtrb9RLrFkp%2FYtI2vz15W9UiyCPHR63QS1%2Fimg.png\" width = \"500\"/></center>\n",
    "\n",
    "* Skip-Gram의 loss-function은 위와 같다. 정답 label들과의 cross entropy error의 합을 사용한 것을 확인할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between CBOW and Skip-gram  \n",
    "\n",
    "* **CBOW tries to predict the target word from its surrounding context words**  \n",
    "* **Skip-gram does the opposite: It tries to predict the surrounding context words from the target word**  \n",
    "\n",
    "보통, Skip-Gram을 CBOW에 비해 선호한다.  \n",
    "이는, Skip-Gram이 big dataset에서 CBOW보다 word vector의 quality가 좋고, 특히 infrequent word에 대해 학습이 정확히 이루어지기 때문이다.  \n",
    "\n",
    "이는, 위의 algorithm을 보면 알 수 있듯이, Skip-Gram은 하나의 token vector과 context token vector가 독립적인 하나의 model로 생각할 수 있는 반면, CBOW는 average를 취하기 때문이다. Average를 취함으로써 주변 상황에 대한 학습이 덜 정확하게 일어나며, 특히 infrequent data에 대해서 gradient도 average된 결과가 update된다.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec training optimization\n",
    "\n",
    "#### 1. Negative Sampling  \n",
    "\n",
    "* 매 학습마다 W, W'을 모두 변경하는 것은 computional complexity가 너무 크다.  \n",
    "Word2Vec의 loss function을 살펴보면, W'의 모든 행들에 대해 연산을 진행한다.\n",
    "* 이는 너무 비효율적이므로 각 token에 대하여 positive sample(기준 단어 주변에 등장), negative sample(기준 단어 주변에 등장하지 않는 단어) 중에서 일부(model마다 다르지만, 보통 5~20개)에 대한 weight만 update를 하자. (softmax 계산 등 모든 계산을 일부에 대해서만 진행)  \n",
    "negative sample은 corpus에서 빈도수가 높은 단어가 뽑히도록 설계되어 있으며, 아래와 같은 probability distribution에서 sampling한다.  \n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbLWoqW%2Fbtrb81Oj2ne%2F1Fo7irUxJDFfUuaMdJigyK%2Fimg.png\" width = \"500\"/></center>  \n",
    "(f(w_i) = frequency of word i)\n",
    "\n",
    "#### 2. Subsampling\n",
    "\n",
    "* 고빈도 단어는 sampling될때마다 모두 학습시키는 것은 비효율적이다. 이를 통해 제외할 확률은 아래와 같다.  \n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqbKmJ%2FbtqCaftgQIT%2FiCwhoikPHpOXvGGrwbLhrk%2Fimg.png\" width = \"500\"/></center>  \n",
    "f(w_i)가 매우 작으면 P(w_i)가 매우 작아져, 항상 학습을 진행하도록 설계함을 확인할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Word2Vec\n",
    "\n",
    "* Vector analysis \n",
    "Word2Vec의 가장 중요한 성질은 vector로 단어 사이의 관계를 표현할 수 있다는 것이다.  \n",
    "  * Cosine Simularity  \n",
    "두 vector 사이의 각도를 내적을 통하여 구해, 얼마나 유사한지를 확인할 수 있다.\n",
    "  * Vector arithmetic  \n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YOXHfYy7ophdy52iwIRLSg.png\" width = \"1000\"/></center>  \n",
    "이처럼, 단어들의 연관성이 vector arithmetic을 통해 구해질 수 있음을 확인할 수 있다.  \n",
    "(ex: fast + longer - long = faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gensim is an open source library in python used for NLP\n",
    "from gensim.models import Word2Vec\n",
    "#nltk(Natural Language Tool Kit): Python package for NLP\n",
    "import nltk\n",
    "#should download this in order to tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize: ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "wordpunct_tokenize: ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
    "... two of them.\\n\\nThanks.'''\n",
    "# There can be different types of ways for a corpus to be tokenized\n",
    "print(f'word_tokenize: {word_tokenize(s)}')\n",
    "print(f'wordpunct_tokenize: {wordpunct_tokenize(s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 0.0679759532213211), ('first', 0.03364057093858719), ('about', 0.009391160681843758), ('word2vec', 0.0045030261389911175), ('.', -0.010839175432920456), ('is', -0.023610752075910568), ('the', -0.09575343132019043), ('another', -0.11405028402805328), ('here', -0.11553988605737686)]\n"
     ]
    }
   ],
   "source": [
    "# Assume this is your corpus\n",
    "corpus = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Here is another sentence.\",\n",
    "    \"This sentence is about Word2Vec.\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "# We sometimes lower the capital in order to increase the accuracy. If that is not the case you desire, don't\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "# Initialize and train a Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Now you can get vector embeddings of words in your corpus\n",
    "vector = model.wv['sentence']  # get vector for word 'sentence'\n",
    "\n",
    "# Find the most similar words in the corpus to 'sentence'\n",
    "similar_words = model.wv.most_similar('sentence') \n",
    "print(similar_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding이 이루어진 단어들은 이제 컴퓨터가 해석할 수 있는 vector가 된 것이며, 이제 NN의 input으로 사용될 수 있다.  \n",
    "NLP를 진행하는데에 있어서 **\"Seq2Seq\"** 는 매우 중요한 model이다.  \n",
    "Seq2Seq는 input으로 sequence로 받고, sequence를 output으로 내보내는 형식의 model이다.\n",
    "* Handling of Variable Length Sequences  \n",
    "Seq2Seq는 input을 sequence의 형태로 받기 때문에, variable length의 input과 output을 다룰 수 있다. 이는 NLP에서 common situation이다.  \n",
    "* Flexibility  \n",
    "Seq2Seq는 NLP의 다양한 task를 해결하는데 사용될 수 있다.  \n",
    "(ex) translation, text summerization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN(Recurrent Neural Network)\n",
    "<img src=\"https://velog.velcdn.com/images%2Fyuns_u%2Fpost%2Fccbb28ea-fa08-4d23-804e-419e6f578e4b%2Fimage.png\" width = \"1000\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN(Recurent Neural Network)는 연속형 data를 처리하기 위해 고안된 가장 초기의 Seq2Seq model이다.  \n",
    "Input으로 x를 받고, output으로 o를 내보내는 단순한 model이지만, x와 o가 t에 대한 함수라는 점이 중요하다.  \n",
    "시간에 따라 다른 input이 입력으로 주어지고, 이에 해당되는 output 또한 시간에 대한 함수로 나타난다.  \n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbluYYw%2Fbtrc8gqQ4AC%2FdcdZgDUngPUE1bn8lx9QQk%2Fimg.png\" width = \"500\"/></center>  \n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkWKIM%2FbtrdeXpN8Mr%2FkfubJ1hsFIqXjL9RNXAQmK%2Fimg.png\" width = \"300\"/></center>  \n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FypP5N%2FbtrdcP68bo3%2FSF6QGDGMmMdh4KkoOoPmzK%2Fimg.png\" width = \"300\"/></center>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 위의 그림과 같이 구성된다. RNN은 크게 3가지의 matrix를 train한다.\n",
    "* **Input-to-Hidden Weights**  \n",
    "  input을 hidden state으로 변환시키는 matrix(U)  \n",
    "* **Hidden-to-Hidden Weights**  \n",
    "  이전 step의 hidden state를 사용하여 현재의 hidden state를 update하는데 사용(W)  \n",
    "* **Hidden-to-Output Weights**  \n",
    "  Hidden state를 output으로 변환시키는 matrix(V)  \n",
    "\n",
    "위 그림을 보면 알 수 있듯이, hidden state(s_t)으로의 변환에는 현재의 input과 과거 step의 hidden state가 동시에 활용된다. Activation function으로는 tanh을 사용하였지만, 이는 조정가능하다.  \n",
    "\n",
    "이를 통해 RNN이 잠시동안 이전의 data를 기억하고 있는 형태로 설계되었음을 확인할 수 있다.  \n",
    "hidden state는 hidden state to output matrix와 곱해진 후 softmax를 통해 normalize가 이루어진다. 이 output은 실제 token의 embedding과 cross entropy를 사용하여 loss function에 기여한다.  \n",
    "\n",
    "t시점까지의 loss function은 이전까지의 cross entropy들의 합으로 표현되며, 각 시점에서는 BPTT(Back Propagation Through Time)가 일어난다. (Back propagation + time dependency)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gradient related problems**\n",
    "  * Gradient vanishing problems  \n",
    "  1보다 작은 gradient를 여러번 곱한다면  \n",
    "  * Gradient exploding problems  \n",
    "  1보다 큰 grdient를 여러번 곱한다면  \n",
    "\n",
    "--> RNN의 구조상 매우 긴 chain에 대해 chain rule이 계산되므로 gradient problem들에 취약하다.  \n",
    "\n",
    "* **Sequence Computation**  \n",
    "RNN은 sequential하게 process가 발생하며, 이는 속도 저하로 연결된다.  \n",
    "\n",
    "--> 이러한 RNN의 문제점 중 gradient 관련 문제들을 해결하기 위해 고안된 방식들이 LSTM와 GRU이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM(Long Short-Term Memory)\n",
    "\n",
    "<img src=\"https://velog.velcdn.com/images/nayeon_p00/post/b38d0650-0ef1-4fa1-a429-d2e1a7ab9f8b/image.png\" width = \"500\"/></center>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LSTM은 RNN의 한 특별한 구조로, gradient problem을 해결하고자 제시된 model이다.  \n",
    "* LSTM은 memory 역할을 하는 cell state(c_t) (위쪽 평행한 직선)와 이전 state의 output인 hidden state (h_t) (아래쪽 평행한 직선)로 구성된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://velog.velcdn.com/images/nayeon_p00/post/dfe5083b-af3f-4940-82b5-0b47887971b5/image.png\" width = \"500\"/></center>  \n",
    "\n",
    "총 4개의 gate가 존재하며, 각각 forget gate(f), input gate(i)와 update gate(g), output gate(o)로 구성된다.  \n",
    "\n",
    "동심원 기호는 Hadamard product를 의미하며, 두개의 크기가 같은 행렬이 주어졌을때, 각 성분끼리를 곱하는 것을 의미한다.  \n",
    "can be denoted as big dot, *, etc..  \n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*54rq3_-FZaJxKLdOYN8qjA.png\" width = \"500\"/></center>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **forget gate**  \n",
    "\n",
    "<img src=\"https://velog.velcdn.com/images/ddochi132/post/bddcf163-3c24-4b36-9652-74a21bbddde2/image.png\" width = \"500\"/></center>  \n",
    "\n",
    "* forget layer에 해당되는 행렬 연산을 진행한 후 이를 이전의 output와 현재의 input과 계산한 후 sigmoid를 통과시켜 f_t를 만든다.  \n",
    "* 이는 이전의 cell state(c_t-1)과 hadamard product를 통해 c_t를 update시킨다.  \n",
    "\n",
    "* 이 층의 의의는 **forget** 이다. 0에 가까운 값과 hadamard product하는 것은 정보를 삭제하는 것을 의마한다. 또한 1에 가까우면 값을 유지함을 의미한다.  \n",
    " 즉 sigmoid의 결과가 0, 1에 가까운 점들이 다수 존재한다는 점을 활용하여 이후 hadamard product를 진행함으로써 forget할 정보와 유지할 정보를 select하는 계층이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **input & update gate**  \n",
    "<img src=\"https://velog.velcdn.com/images/ddochi132/post/f75bc52b-b611-4771-a764-fee7c90fc0d6/image.png\" width = \"500\"/></center>  \n",
    "\n",
    "* update gate(C_t tilda; denoted as g_t in the uppder image .. sorry for notation)는 지난 stage의 output과 현재의 input을 받은 후, cell state를 update하기 위한 vector를 만들어내는 과정이다.  \n",
    " 행렬곱을 진행한 후 tanh을 통과시킨다.  \n",
    "* input gate(i_t)는 update gate의 결과에 대한 scaling factor라고 생각하면 쉽다.  \n",
    " 이후 update gate와 hadamard product를 진행하기 때문이다. forget gate와 마찬가지로 sigmoid를 통과시킨다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **cell state update**  \n",
    "<img src=\"https://velog.velcdn.com/images/ddochi132/post/85c7d527-0e5d-4d28-8915-d7d25ec3f424/image.png\" width = \"500\"/></center>  \n",
    "\n",
    "* 위에서도 설명했듯이, forget gate와 update gate, input gate의 결과를 적용시켜 cell state를 update한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **output gate**  \n",
    "<img src=\"https://velog.velcdn.com/images/ddochi132/post/857755ee-962d-4fc5-918c-0c365deb9bba/image.png\" width = \"500\"/></center>  \n",
    "\n",
    "* next stage의 hidden state가 무엇이 되어야할지를 결정하는 단계이다.  \n",
    "* 현재 memory인 cell state에서 어떤 값이 next hidden state, 즉 current output이 되어야할지를 계산하는 단계  \n",
    "* o_t가 일종의 scaling factor라고 생각하면 된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function은 RNN과 마찬가지 방식을 택한다. 위에서는 예시로 cross-entropy를 들었지만, 해결하고자 하는 문제에 따라 MSE와 같은 다른 loss function을 활용하는 것도 무방하다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Problem??\n",
    "\n",
    "* **The cell state acts like a memory of the LSTM**  \n",
    "gate들은 previous output과 current input을 활용하여 memory를 조작한다. memory는 다시 output을 계산하는데 활용된다.  \n",
    "그렇다면 어떤 점에서 RNN과 달리 gradient problem이 개선되었을까?  \n",
    "\n",
    "  * **Exsistence of Cell state**  \n",
    "    Cell state는 gradient가 이동할 수 있는 일종의 highway 같은 역할을 한다. 이는 gate의 결과가 0과 1과 근접하면 cell state의 연산이 거의 선형에 가까워지므로, back propagation시, vanishing과 exploding이 장시간동안 이루어지지 않을 수 있다.\n",
    "\n",
    "  * **Usage of Gating Mechanism** (**important**)  \n",
    "    gating mechanism을 통해 언제 model이 정보를 forget할지와 remember할지를 control할 수 있다. 이에 따라 과도하게 gradient을 update하는 문제가 덜 발생할 것이며, gradient problem을 줄일 수 있다.\n",
    "\n",
    "* 하지만 LSTM은 RNN의 gradient problem을 해결하기 위하여 굉장히 많은 parameter를 추가시켰으며, model이 heavy해졌다. 이를 해결한 것이 GRU model이다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU(Gated Recurrent Unit)  \n",
    "<img src=\"https://postfiles.pstatic.net/MjAyMDA2MDZfMjU5/MDAxNTkxNDQ1ODAwMDk4.U_avbXLQisbAdpFnWnTdZg-UEJCqh8a9ODl8Sq7mA_Yg.QK3E2jgPAZk-c-AxLxF6DHKpZVxjJaX4cypwQyJSPbEg.PNG.winddori2002/2.PNG?type=w773\" width = \"700\"/></center>  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GRU는 cell state, hidden state가 나누어져 있지 않고, 하나의 hidden state로 표현된다.  \n",
    "* LSTM이 3개의 gate를 가졌던 것에 비해 GRU는 reset gate, update gate로 2개의 gate를 가진다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reset gate(r)  \n",
    "  * (3)과 같이 새롭게 state를 계산할 때 얼마나 이전의 hidden state를 기억할지를 나타내는 scaling factor  \n",
    "\n",
    "* update gate(z)  \n",
    "  * 새로운 hidden state를 얼마나 update할지를 나타내는 parameter  \n",
    "  (4)를 보면 알 수 있듯이, 이전의 state를 얼마나 유지하고 새로운 state로 얼마나 update할지를 표현한다.  \n",
    "\n",
    "* hidden state update  \n",
    "  * (4)의 식을 통하여 hidden state를 update한다.\n",
    "\n",
    "--> Loss function은 LSTM의 경우와 동일하다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
